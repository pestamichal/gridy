"""
Zoptymalizowany loader danych do HBase dla social media data
Skupia siƒô na maksymalnej wydajno≈õci HBase
"""

import happybase
import csv
import time
import logging
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import json

# Konfiguracja logowania
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedHBaseLoader:
    """
    Zoptymalizowana klasa do ≈Çadowania danych social media do HBase
    z fokusem na wydajno≈õƒá i skalowanie
    """
    
    def __init__(self, hbase_host='172.104.141.218', hbase_port=9090):
        self.hbase_host = hbase_host
        self.hbase_port = hbase_port
        self.connection_pool = []
        self.performance_metrics = {
            'load_time': 0,
            'records_per_second': 0,
            'total_records': 0,
            'batch_times': [],
            'connection_time': 0
        }
        
        # Optymalizowane ustawienia
        self.batch_size = 5000  # Wiƒôksze batche dla lepszej wydajno≈õci
        self.thread_pool_size = 8  # WielowƒÖtkowo≈õƒá
        self.auto_flush = False  # Wy≈ÇƒÖczenie auto-flush dla batchy
        
    def create_connection_pool(self, pool_size=4):
        """Tworzenie puli po≈ÇƒÖcze≈Ñ dla wielowƒÖtkowo≈õci"""
        start_time = time.time()
        
        try:
            for i in range(pool_size):
                conn = happybase.Connection(
                    host=self.hbase_host,
                    port=self.hbase_port,
                    autoconnect=False,
                    compat='0.98',  # Kompatybilno≈õƒá dla lepszej wydajno≈õci
                    transport='buffered',  # Buforowane po≈ÇƒÖczenia
                )
                conn.open()
                self.connection_pool.append(conn)
                logger.info(f"‚úÖ Utworzono po≈ÇƒÖczenie {i+1}/{pool_size}")
            
            self.performance_metrics['connection_time'] = time.time() - start_time
            logger.info(f"üîó Pula {pool_size} po≈ÇƒÖcze≈Ñ utworzona w {self.performance_metrics['connection_time']:.2f}s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd tworzenia puli po≈ÇƒÖcze≈Ñ: {e}")
            return False
    
    def setup_optimized_table(self, table_name='social_media_optimized'):
        """
        Utworzenie zoptymalizowanej struktury tabeli HBase
        Z wieloma rodzinami kolumn dla lepszej wydajno≈õci
        """
        try:
            # U≈ºywamy pierwszego po≈ÇƒÖczenia do operacji administracyjnych
            admin_conn = self.connection_pool[0]
            
            # Definicja rodzin kolumn zoptymalizowana pod dane social media
            families = {
                # Podstawowe informacje o po≈õcie
                'post': {
                    'max_versions': 1,
                    'compression': 'SNAPPY',  # Kompresja dla oszczƒôdno≈õci miejsca
                    'bloom_filter_type': 'ROW',  # Bloom filter dla szybszego wyszukiwania
                    'block_cache_enabled': True,
                    'time_to_live': 31536000  # 1 rok TTL
                },
                # Metryki zaanga≈ºowania
                'metrics': {
                    'max_versions': 1,
                    'compression': 'SNAPPY',
                    'bloom_filter_type': 'ROW',
                    'block_cache_enabled': True,
                    'in_memory': True  # Czƒôsto u≈ºywane dane w pamiƒôci
                },
                # Informacje o audience
                'audience': {
                    'max_versions': 1,
                    'compression': 'SNAPPY',
                    'bloom_filter_type': 'ROW'
                },
                # Kampanie i influencerzy
                'campaign': {
                    'max_versions': 1,
                    'compression': 'SNAPPY',
                    'bloom_filter_type': 'ROW'
                }
            }
            
            # Usuniƒôcie tabeli je≈õli istnieje
            if table_name.encode() in admin_conn.tables():
                logger.info(f"üóëÔ∏è Usuwanie istniejƒÖcej tabeli {table_name}")
                admin_conn.delete_table(table_name, disable=True)
            
            # Utworzenie nowej tabeli
            admin_conn.create_table(table_name, families)
            
            logger.info(f"üìä Utworzono zoptymalizowanƒÖ tabelƒô HBase: {table_name}")
            logger.info(f"üìã Rodziny kolumn: {list(families.keys())}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd tworzenia tabeli: {e}")
            return False
    
    def generate_optimized_row_key(self, row):
        """
        Generowanie zoptymalizowanego klucza wiersza
        U≈ºywa hash + timestamp dla lepszej dystrybucji
        """
        platform = row.get('Platform', 'unknown')
        post_id = row.get('Post ID', '')
        timestamp = row.get('Post Timestamp', '')
        
        # Parsowanie timestamp dla lepszego sortowania
        try:
            if timestamp:
                # Zak≈ÇadajƒÖc format typu "2024-01-15 10:30:00"
                dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
                ts_prefix = dt.strftime('%Y%m%d')
            else:
                ts_prefix = '20240101'
        except:
            ts_prefix = '20240101'
        
        # Hash dla lepszej dystrybucji (unikanie hotspot√≥w)
        hash_suffix = hashlib.md5(f"{platform}_{post_id}".encode()).hexdigest()[:8]
        
        # Format: YYYYMMDD_PLATFORM_HASH
        # To zapewnia chronologiczne sortowanie i r√≥wnomiernƒÖ dystrybucjƒô
        row_key = f"{ts_prefix}_{platform}_{hash_suffix}"
        
        return row_key
    
    def prepare_optimized_data(self, row):
        """
        Przygotowanie danych w zoptymalizowanej strukturze
        Podzia≈Ç na logiczne rodziny kolumn
        """
        data = {}
        
        # Rodzina 'post' - podstawowe informacje
        post_data = {
            'post:id': str(row.get('Post ID', '')),
            'post:type': str(row.get('Post Type', '')),
            'post:content': str(row.get('Post Content', ''))[:1000],  # Ograniczenie d≈Çugo≈õci
            'post:timestamp': str(row.get('Post Timestamp', '')),
            'post:platform': str(row.get('Platform', ''))
        }
        
        # Rodzina 'metrics' - metryki zaanga≈ºowania (konwersja na int gdzie mo≈ºliwe)
        metrics_data = {
            'metrics:likes': str(self._safe_int(row.get('Likes', 0))),
            'metrics:comments': str(self._safe_int(row.get('Comments', 0))),
            'metrics:shares': str(self._safe_int(row.get('Shares', 0))),
            'metrics:impressions': str(self._safe_int(row.get('Impressions', 0))),
            'metrics:reach': str(self._safe_int(row.get('Reach', 0))),
            'metrics:engagement_rate': str(self._safe_float(row.get('Engagement Rate', 0.0)))
        }
        
        # Rodzina 'audience' - informacje o odbiorach
        audience_data = {
            'audience:age': str(row.get('Audience Age', '')),
            'audience:gender': str(row.get('Audience Gender', '')),
            'audience:location': str(row.get('Audience Location', '')),
            'audience:interests': str(row.get('Audience Interests', ''))
        }
        
        # Rodzina 'campaign' - kampanie i influencerzy
        campaign_data = {
            'campaign:id': str(row.get('Campaign ID', '')),
            'campaign:sentiment': str(row.get('Sentiment', '')),
            'campaign:influencer_id': str(row.get('Influencer ID', ''))
        }
        
        # ≈ÅƒÖczenie wszystkich danych
        for family_data in [post_data, metrics_data, audience_data, campaign_data]:
            for key, value in family_data.items():
                if value and value != 'nan':  # Filtrowanie pustych warto≈õci
                    data[key.encode()] = value.encode()
        
        return data
    
    def _safe_int(self, value):
        """Bezpieczna konwersja na int"""
        try:
            if value == '' or value is None or str(value).lower() == 'nan':
                return 0
            return int(float(value))
        except:
            return 0
    
    def _safe_float(self, value):
        """Bezpieczna konwersja na float"""
        try:
            if value == '' or value is None or str(value).lower() == 'nan':
                return 0.0
            return float(value)
        except:
            return 0.0
    
    def load_batch_threaded(self, connection, table_name, batch_data):
        """
        ≈Åadowanie pojedynczego batcha w osobnym wƒÖtku
        """
        start_time = time.time()
        
        try:
            table = connection.table(table_name)
            
            # U≈ºycie batch z wiƒôkszym buforem
            with table.batch(batch_size=len(batch_data), transaction=False) as batch:
                for row_key, data in batch_data:
                    batch.put(row_key.encode(), data)
            
            batch_time = time.time() - start_time
            logger.info(f"üì¶ Batch {len(batch_data)} rekord√≥w za≈Çadowany w {batch_time:.3f}s")
            
            return {
                'success': True,
                'records': len(batch_data),
                'time': batch_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd ≈Çadowania batcha: {e}")
            return {
                'success': False,
                'records': 0,
                'time': 0
            }
    
    def load_data_optimized(self, csv_file_path, table_name='social_media_optimized'):
        """
        G≈Ç√≥wna funkcja ≈Çadowania z optymalizacjami:
        - WielowƒÖtkowo≈õƒá
        - Wiƒôksze batche
        - Zoptymalizowana struktura danych
        - Monitoring wydajno≈õci
        """
        logger.info("üöÄ Rozpoczƒôcie zoptymalizowanego ≈Çadowania do HBase...")
        start_time = time.time()
        
        total_records = 0
        current_batch = []
        batch_futures = []
        
        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                
                # Sprawdzenie nag≈Ç√≥wk√≥w
                logger.info(f"üìã Kolumny CSV: {reader.fieldnames}")
                
                # Przygotowanie ThreadPoolExecutor
                with ThreadPoolExecutor(max_workers=self.thread_pool_size) as executor:
                    
                    for row in reader:
                        total_records += 1
                        
                        # Przygotowanie danych
                        row_key = self.generate_optimized_row_key(row)
                        optimized_data = self.prepare_optimized_data(row)
                        
                        current_batch.append((row_key, optimized_data))
                        
                        # Wys≈Çanie batcha gdy osiƒÖgnie odpowiedni rozmiar
                        if len(current_batch) >= self.batch_size:
                            # Wyb√≥r po≈ÇƒÖczenia z puli (round-robin)
                            connection = self.connection_pool[len(batch_futures) % len(self.connection_pool)]
                            
                            # Wys≈Çanie batcha w osobnym wƒÖtku
                            future = executor.submit(
                                self.load_batch_threaded,
                                connection,
                                table_name,
                                current_batch.copy()
                            )
                            batch_futures.append(future)
                            
                            logger.info(f"üì§ Wys≈Çano batch {len(batch_futures)} ({len(current_batch)} rekord√≥w)")
                            current_batch = []
                            
                            # Monitoring postƒôpu co 10 batchy
                            if len(batch_futures) % 10 == 0:
                                logger.info(f"üìä Postƒôp: {total_records} rekord√≥w, {len(batch_futures)} batchy w toku")
                    
                    # Wys≈Çanie ostatniego batcha
                    if current_batch:
                        connection = self.connection_pool[0]
                        future = executor.submit(
                            self.load_batch_threaded,
                            connection,
                            table_name,
                            current_batch
                        )
                        batch_futures.append(future)
                        logger.info(f"üì§ Wys≈Çano ostatni batch ({len(current_batch)} rekord√≥w)")
                    
                    # Oczekiwanie na zako≈Ñczenie wszystkich batchy
                    logger.info("‚è≥ Oczekiwanie na zako≈Ñczenie wszystkich batchy...")
                    successful_batches = 0
                    total_batch_time = 0
                    
                    for future in as_completed(batch_futures):
                        result = future.result()
                        if result['success']:
                            successful_batches += 1
                            total_batch_time += result['time']
                            self.performance_metrics['batch_times'].append(result['time'])
        
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas ≈Çadowania: {e}")
            return False
        
        # Obliczenie metryk wydajno≈õci
        total_time = time.time() - start_time
        self.performance_metrics.update({
            'load_time': total_time,
            'total_records': total_records,
            'records_per_second': total_records / total_time if total_time > 0 else 0,
            'successful_batches': successful_batches,
            'total_batches': len(batch_futures),
            'avg_batch_time': total_batch_time / successful_batches if successful_batches > 0 else 0
        })
        
        # Raport wydajno≈õci
        logger.info("="*60)
        logger.info("üìä RAPORT WYDAJNO≈öCI ≈ÅADOWANIA HBASE")
        logger.info("="*60)
        logger.info(f"‚úÖ Za≈Çadowano rekord√≥w: {total_records:,}")
        logger.info(f"‚è±Ô∏è Ca≈Çkowity czas: {total_time:.2f}s")
        logger.info(f"üöÄ Wydajno≈õƒá: {self.performance_metrics['records_per_second']:,.1f} rekord√≥w/s")
        logger.info(f"üì¶ Batche: {successful_batches}/{len(batch_futures)} udanych")
        logger.info(f"‚ö° ≈öredni czas batcha: {self.performance_metrics['avg_batch_time']:.3f}s")
        logger.info(f"üîó Czas po≈ÇƒÖcze≈Ñ: {self.performance_metrics['connection_time']:.2f}s")
        logger.info("="*60)
        
        return True
    
    def verify_data_loading(self, table_name='social_media_optimized', sample_size=10):
        """Weryfikacja poprawno≈õci za≈Çadowanych danych"""
        logger.info("üîç Weryfikacja za≈Çadowanych danych...")
        
        try:
            table = self.connection_pool[0].table(table_name)
            
            # Pobranie pr√≥bki danych
            sample_count = 0
            for key, data in table.scan(limit=sample_size):
                sample_count += 1
                logger.info(f"üìù Rekord {sample_count}: {key.decode()}")
                
                # Pokazanie przyk≈Çadowych danych z ka≈ºdej rodziny kolumn
                for family in ['post', 'metrics', 'audience', 'campaign']:
                    family_data = {k.decode(): v.decode() for k, v in data.items() 
                                 if k.decode().startswith(family)}
                    if family_data:
                        logger.info(f"   {family}: {dict(list(family_data.items())[:3])}")
            
            # Szybki count (przybli≈ºony)
            logger.info("üìä Liczenie rekord√≥w w tabeli...")
            count = 0
            for _ in table.scan():
                count += 1
                if count % 10000 == 0:
                    logger.info(f"   Policzono: {count:,} rekord√≥w...")
                if count > 100000:  # Limit dla szybko≈õci
                    logger.info(f"   Przerwano liczenie na {count:,} rekordach")
                    break
            
            logger.info(f"‚úÖ Weryfikacja zako≈Ñczona. Pr√≥bka: {sample_count}, Policzono: {count:,}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd weryfikacji: {e}")
            return False
    
    def cleanup(self):
        """Zamkniƒôcie wszystkich po≈ÇƒÖcze≈Ñ"""
        for conn in self.connection_pool:
            try:
                conn.close()
            except:
                pass
        logger.info("üîí Zamkniƒôto wszystkie po≈ÇƒÖczenia")
    
    def get_performance_report(self):
        """Zwr√≥cenie raportu wydajno≈õci"""
        return self.performance_metrics

def main():
    """Przyk≈Çad u≈ºycia zoptymalizowanego loadera"""
    
    # ≈öcie≈ºka do pliku CSV
    csv_file_path = 'social_media_engagement_data.csv'
    table_name = 'social_media_optimized'
    
    # Utworzenie loadera
    loader = OptimizedHBaseLoader()
    
    try:
        # Utworzenie puli po≈ÇƒÖcze≈Ñ
        if not loader.create_connection_pool(pool_size=4):
            return
        
        # Konfiguracja tabeli
        if not loader.setup_optimized_table(table_name):
            return
        
        # ≈Åadowanie danych
        if loader.load_data_optimized(csv_file_path, table_name):
            # Weryfikacja
            loader.verify_data_loading(table_name)
            
            # Zapisanie raportu wydajno≈õci
            report = loader.get_performance_report()
            with open('hbase_performance_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info("‚úÖ Optymalizowane ≈Çadowanie zako≈Ñczone pomy≈õlnie!")
        else:
            logger.error("‚ùå ≈Åadowanie nie powiod≈Ço siƒô")
            
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd podczas eksperymentu: {e}")
    finally:
        loader.cleanup()

if __name__ == "__main__":
    main()