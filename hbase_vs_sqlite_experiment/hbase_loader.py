"""
Zoptymalizowany loader danych do HBase dla social media data
Wersja z poprawkami dla problem√≥w wielowƒÖtkowych i broken pipe
"""

import happybase
import csv
import time
import logging
import hashlib
from datetime import datetime
import json
import threading
from queue import Queue
import socket

# Konfiguracja logowania
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedHBaseLoader:
    """
    Zoptymalizowana klasa do ≈Çadowania danych social media do HBase
    z poprawkami dla problem√≥w wielowƒÖtkowych
    """
    
    def __init__(self, hbase_host='172.104.141.218', hbase_port=9090):
        self.hbase_host = hbase_host
        self.hbase_port = hbase_port
        self.main_connection = None
        self.performance_metrics = {
            'load_time': 0,
            'records_per_second': 0,
            'total_records': 0,
            'batch_times': [],
            'connection_time': 0
        }
        
        # Konserwatywne ustawienia dla stabilno≈õci
        self.batch_size = 2000  # Mniejsze batche dla stabilno≈õci
        self.max_workers = 3    # Mniej wƒÖtk√≥w
        self.connection_timeout = 30
        self.socket_timeout = 60
        self.max_retries = 3
        
        # Kontrola wƒÖtk√≥w
        self.batch_queue = Queue()
        self.results_queue = Queue()
        self.stop_threads = threading.Event()
        
    def create_single_connection(self):
        """Utworzenie pojedynczego, stabilnego po≈ÇƒÖczenia"""
        start_time = time.time()
        
        try:
            # Konfiguracja socket timeout
            socket.setdefaulttimeout(self.socket_timeout)
            
            self.main_connection = happybase.Connection(
                host=self.hbase_host,
                port=self.hbase_port,
                autoconnect=False,
                compat='0.98',
            )
            
            # Test po≈ÇƒÖczenia
            self.main_connection.open()
            tables = self.main_connection.tables()
            logger.info(f"‚úÖ Po≈ÇƒÖczono z HBase. Dostƒôpne tabele: {len(tables)}")
            
            self.performance_metrics['connection_time'] = time.time() - start_time
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd po≈ÇƒÖczenia z HBase: {e}")
            return False
    
    def setup_optimized_table(self, table_name='social_media_optimized'):
        """Utworzenie zoptymalizowanej struktury tabeli HBase"""
        try:
            # Sprawdzenie czy tabela istnieje i jej usuniƒôcie dla ≈õwie≈ºego startu
            existing_tables = self.main_connection.tables()
            
            if table_name.encode() in existing_tables:
                logger.info(f"üóëÔ∏è Usuwanie istniejƒÖcej tabeli {table_name}")
                self.main_connection.delete_table(table_name, disable=True)
                time.sleep(3)  # Oczekiwanie na usuniƒôcie
            
            # Uproszczona struktura dla stabilno≈õci - tylko dwie rodziny kolumn
            families = {
                # Podstawowe dane
                'cf': {  # Zmiana na 'cf' - standardowa nazwa
                    'max_versions': 1,
                    'compression': 'NONE',  # Wy≈ÇƒÖczenie kompresji dla stabilno≈õci
                    'bloom_filter_type': 'ROW',
                    'block_cache_enabled': True
                },
                # Metryki
                'metrics': {
                    'max_versions': 1,
                    'compression': 'NONE',
                    'bloom_filter_type': 'ROW',
                    'block_cache_enabled': True
                }
            }
            
            # Utworzenie tabeli
            self.main_connection.create_table(table_name, families)
            logger.info(f"üìä Utworzono tabelƒô HBase: {table_name}")
            
            # Kr√≥tkie oczekiwanie na utworzenie tabeli
            time.sleep(2)
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd tworzenia tabeli: {e}")
            return False
    
    def generate_simple_row_key(self, row, counter):
        """Generowanie prostego, unikalnego klucza wiersza"""
        platform = row.get('Platform', 'unknown')[:10]
        post_id = str(row.get('Post ID', ''))
        
        # WyciƒÖgniƒôcie tylko pierwszych 8 znak√≥w z Post ID dla skr√≥cenia
        if len(post_id) > 8:
            post_id_short = post_id[:8]
        else:
            post_id_short = post_id
        
        # Format: PLATFORM_POSTID_COUNTER
        row_key = f"{platform}_{post_id_short}_{counter:08d}"
        
        return row_key
    
    def prepare_simple_data(self, row):
        """Przygotowanie danych w uproszczonej strukturze - dostosowane do rzeczywistego CSV"""
        data = {}
        
        # Rodzina 'cf' - wszystkie podstawowe informacje
        # Mapowanie dok≈Çadnie na kolumny z przyk≈Çadowych danych
        basic_mapping = {
            'Platform': 'platform',
            'Post ID': 'post_id', 
            'Post Type': 'post_type',
            'Post Content': 'post_content',
            'Post Timestamp': 'post_timestamp',
            'Audience Age': 'audience_age',
            'Audience Gender': 'audience_gender', 
            'Audience Location': 'audience_location',
            'Audience Interests': 'audience_interests',
            'Campaign ID': 'campaign_id',
            'Sentiment': 'sentiment',
            'Influencer ID': 'influencer_id'
        }
        
        for csv_field, hbase_field in basic_mapping.items():
            value = str(row.get(csv_field, '')).strip()
            if value and value.lower() not in ['nan', 'none', '']:
                # Specjalne ograniczenia dla r√≥≈ºnych typ√≥w p√≥l
                if csv_field == 'Post Content':
                    value = value[:500]  # Ograniczenie tre≈õci posta
                elif csv_field == 'Audience Interests':
                    value = value[:200]  # Ograniczenie listy zainteresowa≈Ñ
                elif csv_field == 'Audience Location':
                    value = value[:100]  # Ograniczenie lokalizacji
                else:
                    value = value[:50]   # Standardowe ograniczenie
                
                data[f'cf:{hbase_field}'] = value
        
        # Rodzina 'metrics' - pola numeryczne
        # Mapowanie metryk z odpowiedniƒÖ konwersjƒÖ
        metrics_mapping = {
            'Likes': 'likes',
            'Comments': 'comments', 
            'Shares': 'shares',
            'Impressions': 'impressions',
            'Reach': 'reach',
            'Engagement Rate': 'engagement_rate'
        }
        
        for csv_field, hbase_field in metrics_mapping.items():
            value = self._safe_convert(row.get(csv_field, '0'))
            if value is not None:
                data[f'metrics:{hbase_field}'] = str(value)
        
        # Konwersja na bytes tylko je≈õli mamy jakie≈õ dane
        if not data:
            return {}
            
        byte_data = {}
        for key, value in data.items():
            if value and str(value).strip():
                try:
                    byte_data[key.encode('utf-8')] = str(value).encode('utf-8')
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Problem z kodowaniem {key}: {e}")
                    continue
        
        return byte_data
    
    def _safe_convert(self, value):
        """Bezpieczna konwersja warto≈õci - dostosowana do rzeczywistych danych"""
        if not value or str(value).strip() in ['', 'nan', 'None', 'null']:
            return 0
        
        # Konwersja string na liczbƒô
        value_str = str(value).strip()
        
        try:
            # Pr√≥ba konwersji na float
            float_val = float(value_str)
            
            # Je≈õli to liczba ca≈Çkowita, zwr√≥ƒá int
            if float_val.is_integer() and abs(float_val) < 2147483647:  # Limit int32
                return int(float_val)
            else:
                # ZaokrƒÖglenie float do 2 miejsc po przecinku
                return round(float_val, 2)
                
        except ValueError:
            # Je≈õli konwersja siƒô nie uda≈Ça, zwr√≥ƒá 0
            logger.debug(f"Nie mo≈ºna skonwertowaƒá '{value_str}' na liczbƒô")
            return 0
        except Exception as e:
            logger.warning(f"Nieoczekiwany b≈ÇƒÖd konwersji '{value_str}': {e}")
            return 0
    
    def load_batch_with_retry(self, table_name, batch_data, batch_id):
        """≈Åadowanie batcha z mechanizmem retry"""
        
        for attempt in range(self.max_retries):
            try:
                start_time = time.time()
                
                if attempt > 0:
                    logger.info(f"üîÑ Batch {batch_id} - pr√≥ba {attempt + 1}")
                    time.sleep(2 * attempt)  # Exponential backoff
                
                # U≈ºycie g≈Ç√≥wnego po≈ÇƒÖczenia
                table = self.main_connection.table(table_name)
                
                # U≈ºycie batch API zamiast pojedynczych put - bardziej efektywne
                success_count = 0
                batch_size = min(500, len(batch_data))  # Mniejsze micro-batche
                
                for i in range(0, len(batch_data), batch_size):
                    micro_batch = batch_data[i:i + batch_size]
                    
                    try:
                        # U≈ºycie batch context manager
                        with table.batch(batch_size=len(micro_batch)) as batch:
                            for row_key, data in micro_batch:
                                batch.put(row_key, data)  # Bez .encode() - ju≈º jest string
                        
                        success_count += len(micro_batch)
                        
                    except Exception as micro_error:
                        logger.warning(f"‚ö†Ô∏è B≈ÇƒÖd micro-batch {i//batch_size + 1}: {micro_error}")
                        
                        # Fallback na pojedyncze operacje
                        for row_key, data in micro_batch:
                            try:
                                table.put(row_key, data)
                                success_count += 1
                            except Exception as single_error:
                                logger.warning(f"‚ö†Ô∏è B≈ÇƒÖd pojedynczego rekordu {row_key}: {single_error}")
                                continue
                
                batch_time = time.time() - start_time
                
                if success_count > 0:
                    logger.info(f"‚úÖ Batch {batch_id}: {success_count}/{len(batch_data)} rekord√≥w w {batch_time:.2f}s")
                    return {
                        'success': True,
                        'batch_id': batch_id,
                        'records': success_count,
                        'time': batch_time,
                        'attempt': attempt + 1
                    }
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Batch {batch_id} pr√≥ba {attempt + 1} nieudana: {e}")
                if attempt == self.max_retries - 1:
                    logger.error(f"‚ùå Batch {batch_id} ostatecznie nieudany po {self.max_retries} pr√≥bach")
        
        return {
            'success': False,
            'batch_id': batch_id,
            'records': 0,
            'time': 0,
            'attempt': self.max_retries
        }
    
    def load_data_sequential(self, csv_file_path, table_name='social_media_optimized'):
        """
        Sekwencyjne ≈Çadowanie danych - bardziej stabilne ni≈º wielowƒÖtkowe
        """
        logger.info("üöÄ Rozpoczƒôcie sekwencyjnego ≈Çadowania do HBase...")
        start_time = time.time()
        
        total_records = 0
        successful_records = 0
        batch_count = 0
        current_batch = []
        
        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                
                logger.info(f"üìã Kolumny CSV: {reader.fieldnames}")
                
                for row_idx, row in enumerate(reader):
                    total_records += 1
                    
                    # Przygotowanie danych
                    row_key = self.generate_simple_row_key(row, row_idx)
                    data = self.prepare_simple_data(row)
                    
                    if data:  # Tylko je≈õli sƒÖ jakie≈õ dane
                        current_batch.append((row_key, data))
                    
                    # Wys≈Çanie batcha
                    if len(current_batch) >= self.batch_size:
                        batch_count += 1
                        result = self.load_batch_with_retry(table_name, current_batch, batch_count)
                        
                        if result['success']:
                            successful_records += result['records']
                            self.performance_metrics['batch_times'].append(result['time'])
                        
                        current_batch = []
                        
                        # Monitoring postƒôpu
                        if batch_count % 5 == 0:
                            elapsed = time.time() - start_time
                            rate = successful_records / elapsed if elapsed > 0 else 0
                            logger.info(f"üìä Postƒôp: {successful_records:,}/{total_records:,} rekord√≥w, {rate:.1f} rek/s")
                
                # Ostatni batch
                if current_batch:
                    batch_count += 1
                    result = self.load_batch_with_retry(table_name, current_batch, batch_count)
                    if result['success']:
                        successful_records += result['records']
        
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas ≈Çadowania: {e}")
            return False
        
        # Obliczenie metryk
        total_time = time.time() - start_time
        self.performance_metrics.update({
            'load_time': total_time,
            'total_records': total_records,
            'successful_records': successful_records,
            'records_per_second': successful_records / total_time if total_time > 0 else 0,
            'success_rate': (successful_records / total_records * 100) if total_records > 0 else 0,
            'total_batches': batch_count
        })
        
        # Raport
        logger.info("="*60)
        logger.info("üìä RAPORT ≈ÅADOWANIA HBASE")
        logger.info("="*60)
        logger.info(f"üìù Przetworzonych rekord√≥w: {total_records:,}")
        logger.info(f"‚úÖ Za≈Çadowanych rekord√≥w: {successful_records:,}")
        logger.info(f"üìà Skuteczno≈õƒá: {self.performance_metrics['success_rate']:.1f}%")
        logger.info(f"‚è±Ô∏è Ca≈Çkowity czas: {total_time:.2f}s")
        logger.info(f"üöÄ Wydajno≈õƒá: {self.performance_metrics['records_per_second']:,.1f} rek/s")
        logger.info(f"üì¶ Liczba batchy: {batch_count}")
        logger.info("="*60)
        
        return successful_records > 0
    
    def verify_data_simple(self, table_name='social_media_optimized', sample_size=5):
        """Prosta weryfikacja za≈Çadowanych danych - dostosowana do struktury"""
        logger.info("üîç Weryfikacja danych...")
        
        try:
            table = self.main_connection.table(table_name)
            
            count = 0
            logger.info("üìã Pr√≥bka za≈Çadowanych danych:")
            
            for key, data in table.scan(limit=sample_size):
                count += 1
                row_key = key.decode('utf-8')
                logger.info(f"\nüìù Rekord {count}: {row_key}")
                
                # Podzia≈Ç na rodziny kolumn dla lepszej czytelno≈õci
                cf_data = {}
                metrics_data = {}
                
                for k, v in data.items():
                    column_name = k.decode('utf-8')
                    column_value = v.decode('utf-8')
                    
                    if column_name.startswith('cf:'):
                        cf_data[column_name] = column_value
                    elif column_name.startswith('metrics:'):
                        metrics_data[column_name] = column_value
                
                # Wy≈õwietlenie podstawowych danych (cf)
                if cf_data:
                    logger.info("   üìÑ Podstawowe dane (cf):")
                    for k, v in list(cf_data.items())[:5]:  # Pierwsze 5
                        logger.info(f"      {k}: {v[:50]}{'...' if len(v) > 50 else ''}")
                
                # Wy≈õwietlenie metryk
                if metrics_data:
                    logger.info("   üìä Metryki:")
                    for k, v in metrics_data.items():
                        logger.info(f"      {k}: {v}")
            
            # Sprawdzenie ca≈Çkowitej liczby rekord√≥w (z limitem dla wydajno≈õci)
            logger.info("\nüìä Liczenie rekord√≥w w tabeli...")
            total_count = 0
            for _ in table.scan():
                total_count += 1
                if total_count % 1000 == 0:
                    logger.info(f"   Policzono: {total_count:,}")
                if total_count >= 10000:  # Limit dla szybko≈õci
                    logger.info(f"   Przerwano liczenie na {total_count:,} rekordach")
                    break
            
            logger.info(f"\n‚úÖ Weryfikacja zako≈Ñczona:")
            logger.info(f"   üìã Sprawdzono pr√≥bkƒô: {count} rekord√≥w")
            logger.info(f"   üìä Policzono w tabeli: {total_count:,} rekord√≥w")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd weryfikacji: {e}")
            return False
    
    def cleanup(self):
        """Zamkniƒôcie po≈ÇƒÖczenia"""
        if self.main_connection:
            try:
                self.main_connection.close()
                logger.info("üîí Zamkniƒôto po≈ÇƒÖczenie z HBase")
            except:
                pass
    
    def get_performance_report(self):
        """Zwr√≥cenie raportu wydajno≈õci"""
        return self.performance_metrics

def main():
    """Przyk≈Çad u≈ºycia stabilnego loadera"""
    
    csv_file_path = '../social_media_engagement_data.csv'
    table_name = 'social_media_optimized'
    
    loader = OptimizedHBaseLoader()
    
    try:
        # Pojedyncze po≈ÇƒÖczenie
        if not loader.create_single_connection():
            return
        
        # Konfiguracja tabeli
        if not loader.setup_optimized_table(table_name):
            return
        
        # Sekwencyjne ≈Çadowanie
        if loader.load_data_sequential(csv_file_path, table_name):
            # Weryfikacja
            loader.verify_data_simple(table_name)
            
            # Raport
            report = loader.get_performance_report()
            with open('hbase_stable_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info("‚úÖ ≈Åadowanie zako≈Ñczone pomy≈õlnie!")
        else:
            logger.error("‚ùå ≈Åadowanie nie powiod≈Ço siƒô")
            
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd: {e}")
    finally:
        loader.cleanup()

if __name__ == "__main__":
    main()